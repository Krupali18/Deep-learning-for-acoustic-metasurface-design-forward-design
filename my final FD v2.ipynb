{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"my final FD v2.ipynb","provenance":[{"file_id":"1E1bO-hAqYEvbQnK-qUhSOXSHmOSp7kZw","timestamp":1638386854720}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"_UdvxmcFXDqy"},"source":["import pandas as pd\n","import numpy as np\n","np.random.seed(123)\n","import glob\n","import os\n","import re\n","import cv2\n","import gc\n","from sklearn.decomposition import PCA\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.layers import concatenate, Dense, Flatten, GlobalAveragePooling2D, Dropout, Conv2D, MaxPool2D, \\\n","    BatchNormalization, Reshape, Input, Lambda\n","from tensorflow.keras.backend import tile\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5TPp3mFnYc0i","executionInfo":{"status":"ok","timestamp":1638457748836,"user_tz":-60,"elapsed":14,"user":{"displayName":"Krupali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4tRgWYAUKNmdN5oeOCOxWnUy5RHfM6x1B5ITwFw=s64","userId":"03950371097643310073"}},"outputId":"647ae312-8c74-450a-e665-7407a66f3346"},"source":["!unzip /content/drive/MyDrive/data.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["unzip:  cannot find or open /content/drive/MyDrive/data.zip, /content/drive/MyDrive/data.zip.zip or /content/drive/MyDrive/data.zip.ZIP.\n"]}]},{"cell_type":"code","metadata":{"id":"brVFoIC5YRUh"},"source":["# input height and width of the image\n","HEIGHT = 64\n","WIDTH = 64\n","\n","# weighted MSE to give more weight to first column\n","def weighted_mse(yTrue,yPred):\n","    ones = K.ones_like(yTrue[0,:]) \n","    idx = K.cumsum(ones)\n","    return K.mean((1/idx)*K.square(yTrue-yPred))\n","\n","# rotate image by a given angle\n","def rotateImage(image, angle):\n","    (h, w) = image.shape[:2]\n","    (cX, cY) = (w // 2, h // 2)\n","\n","    M = cv2.getRotationMatrix2D((cX, cY), angle, 1.0)\n","    cos = np.abs(M[0, 0])\n","    sin = np.abs(M[0, 1])\n","\n","    nW = int((h * sin) + (w * cos))\n","    nH = int((h * cos) + (w * sin))\n","\n","    M[0, 2] += (nW / 2) - cX\n","    M[1, 2] += (nH / 2) - cY\n","\n","    return cv2.warpAffine(image, M, (nW, nH))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYD8vUMOYUJn","executionInfo":{"status":"ok","timestamp":1638457751519,"user_tz":-60,"elapsed":10,"user":{"displayName":"Krupali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4tRgWYAUKNmdN5oeOCOxWnUy5RHfM6x1B5ITwFw=s64","userId":"03950371097643310073"}},"outputId":"8558dbb6-7ba5-420c-86de-18d9660bae93"},"source":["# for each type get images and associated csv files in training/ validation/ test file lists\n","# ensures equal distribution of values in all the 3 sets\n","\n","train_files = []\n","validation_files =[]\n","test_files = []\n","\n","folders = glob.glob(\"data/*\")\n","train_pca = []\n","print(folders)\n","for folder in folders:\n","    folder_files = [x for x in os.listdir(folder) if x.endswith(\".jpg\")]\n","    for file in folder_files:\n","        csv_file = file.replace('.jpg', '.csv')\n","        file = os.path.join(folder, file)\n","        try:\n","            csv_file = os.path.join(folder, csv_file)\n","            csv_file = pd.read_csv(csv_file)\n","            if csv_file.shape[0] == 205:\n","                prob = np.random.random()\n","                if prob < 0.02:\n","                    test_files.append(file)\n","                elif 0.02 < prob < 0.12:\n","                    validation_files.append(file)\n","                else:\n","                    train_files.append(file)\n","        except:\n","            continue\n","\n","print(len(train_files))\n","print(len(validation_files))\n","print(len(test_files))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","0\n","0\n","0\n"]}]},{"cell_type":"code","metadata":{"id":"boqimP2YYZEe"},"source":["data = np.zeros((len(train_files), 201))\n","for i,file in enumerate(train_files):\n","    label_name = file.replace('.jpg', '.csv')\n","    label_file = pd.read_csv(label_name, skiprows=[0, 1, 2, 3, 4], names=['freq', 'values'])\n","    data[i, :] = label_file['values'].astype(float)\n","# using the training files fit pca\n","pca = PCA(n_components=20)\n","pca.fit(data)\n","\n","print(sum(pca.explained_variance_ratio_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrDrkDSWYb3U"},"source":["# batch generator to generate batches\n","def batch_generator(X, batch_size=64):\n","    while True:\n","        # Select files (paths/indices) for the batch\n","        batch_paths = np.random.randint(low=0, high=len(X), size=batch_size)\n","\n","        images = []\n","        props = []\n","        batch_label = []\n","\n","\n","        # Read in each input, perform preprocessing and get labels\n","        for input_path_index in batch_paths:\n","            file= X[input_path_index]\n","            img = cv2.imread(file,0)\n","\n","            folder = os.path.split(os.path.split(file)[0])[1]\n","            if folder == '0d65h' or folder == '0d75h':\n","                angle = np.random.randint(0, 360)\n","                img = rotateImage(img, angle)\n","            img = cv2.resize(img, (HEIGHT, WIDTH))\n","\n","            img = img/255\n","            img = img.reshape(img.shape[0], img.shape[1], 1)\n","            prop = re.findall(\"\\d+\", folder)\n","            prop = [int(x) for x in prop]\n","            prop[0] = prop[0] / 60\n","            prop[1] = prop[1] / 75\n","            prop1 = np.full((HEIGHT, WIDTH, 1), prop[0])\n","            prop2 = np.full((HEIGHT, WIDTH, 1), prop[1])\n","            props = np.concatenate((prop1, prop2), axis=2)\n","            # combine properties to images as additional channels\n","            img = np.concatenate((img, props), axis=2)\n","            images.append(img)\n","            # props.append(prop)\n","            label_name = file.replace('.jpg', '.csv')\n","\n","            label_file = pd.read_csv(label_name, skiprows=[0, 1, 2, 3, 4], names=['freq', 'values'])\n","            label_file['values'] = label_file['values'].astype(float)\n","            label = label_file['values'].values\n","            label = pca.transform(label.reshape(1,-1)).reshape(-1,)\n","            batch_label.append(label)\n","\n","\n","        batch_x = np.array(images)\n","        batch_y = np.array(batch_label)\n","\n","        yield (batch_x, batch_y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gT8_VnqwYlRP"},"source":["# using the segregated files define train and validation generators\n","train_gen = batch_generator(train_files, batch_size=32)\n","valid_gen = batch_generator(validation_files, batch_size=8)\n","\n","# define model architecture\n","def cnn():\n","    model = Sequential()\n","    # add model layers\n","    model.add(Conv2D(16, kernel_size=3, activation='relu', input_shape = (HEIGHT, WIDTH, 3), padding='same', name=\"conv_1\"))\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same', name=\"conv_2\"))\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', name=\"conv_3\"))\n","    model.add(MaxPool2D(pool_size=(2, 2)))\n","    model.add(BatchNormalization())\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(20, activation=\"linear\"))\n","    return model\n","\n","\n","model = cnn()\n","\n","# compile model\n","model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss=weighted_mse, metrics=['mae'])\n","print(model.summary())\n","callbacks = [\n","    tf.keras.callbacks.ModelCheckpoint('best_model_v2.h5', save_weights_only=True, save_best_only=True, mode='min'),\n","    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=100, min_lr=1e-5, epsilon=0.000001, verbose=2,\n","                          mode='min'),\n","]\n","\n","# clear memory\n","tf.keras.backend.clear_session()\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvnhNPcU9Tr7"},"source":["# train the model\n","# model.load_weights('/content/drive/MyDrive/YOLOv4_weight/best_model_v2.h5')\n","\n","hist = model.fit(\n","    train_gen,\n","    epochs=500,\n","    verbose=2,\n","    steps_per_epoch=len(train_files) // 32,\n","    validation_data=valid_gen,\n","    validation_steps=len(validation_files) // 8,\n","    callbacks = callbacks\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCYMjwX9TnRQ"},"source":["# Plot - loss during training\n","plt.figure()\n","plt.plot(hist.history['loss'])\n","plt.plot(hist.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='best')\n","plt.show()\n","\n","plt.figure()\n","plt.plot(hist.history['mae'])\n","plt.plot(hist.history['val_mae'])\n","plt.title('model mae')\n","plt.ylabel('mae')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SSst84ihkZ90"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P3e0Ug2X8dfT"},"source":["# evaluate on test/ unseen data\n","test_gen = batch_generator(test_files, 1)\n","\n","model.load_weights('best_model_v2.h5')\n","print(model.evaluate_generator(test_gen, steps=len(test_files)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0V0iHZmn8UAk"},"source":["# see prediction plots for first 10 responses in test data\n","for file in test_files[10:20]:\n","    img = cv2.imread(file, 0)\n","    folder = os.path.split(os.path.split(file)[0])[1]\n","    if folder == '0d65h' or folder == '0d75h':\n","        angle = np.random.randint(0, 360)\n","        img = rotateImage(img, angle)\n","    img = cv2.resize(img, (HEIGHT, WIDTH))\n","    img = img.reshape(img.shape[0], img.shape[1], 1)\n","    img = img/255\n","    prop = re.findall(\"\\d+\", folder)\n","    prop = [int(x) for x in prop]\n","    prop[0] = prop[0]/60\n","    prop[1] = prop[1]/ 75\n","    prop1 = np.full((HEIGHT, WIDTH, 1), prop[0])\n","    prop2 = np.full((HEIGHT, WIDTH, 1), prop[1])\n","    props = np.concatenate((prop1, prop2), axis=2)\n","    img = np.concatenate((img, props), axis=2)\n","    img = img.reshape(-1, img.shape[0], img.shape[1], img.shape[2])\n","    label_name = file.replace('.jpg', '.csv')\n","\n","    label_file = pd.read_csv(label_name, skiprows=[0, 1, 2, 3, 4], names=['freq', 'values'])\n","    label_file['values'] = label_file['values'].astype(float)\n","    label = label_file['values'].values\n","    pred = model.predict(img)\n","    predictions = pca.inverse_transform(pred[0])\n","    # pred = scale.inverse_transform(predictions)\n","    label_file['pred_values'] = predictions\n","    plt.plot(predictions)\n","    plt.plot(label)\n","    plt.legend(['Predictions', 'Actual_Values'])\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wxfda0Y9AW-9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638457758024,"user_tz":-60,"elapsed":236,"user":{"displayName":"Krupali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4tRgWYAUKNmdN5oeOCOxWnUy5RHfM6x1B5ITwFw=s64","userId":"03950371097643310073"}},"outputId":"4e2a6007-ab5b-4365-b9cb-d609e4f1deb5"},"source":["mv /content/best_model_v2.h5 /content/drive/MyDrive/images_s"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mv: cannot stat '/content/best_model_v2.h5': No such file or directory\n"]}]},{"cell_type":"code","metadata":{"id":"DFKJB9xzLUo4"},"source":[""],"execution_count":null,"outputs":[]}]}